% author(s): Jared Dyreson
% school: California State University Fullerton, 2019
% linear regression material condensed

\documentclass[6pt]{article}

\usepackage[letterpaper]{geometry}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lmodern}
\usepackage{scalerel}
\geometry{top=1.0cm, bottom=1.0cm, left=1.0cm, right=1.0cm}
\pagenumbering{gobble}

% horizontal line spanning the whole page

\newcommand{\HL}{\par\noindent\rule{\textwidth}{0.4pt}}

% square and circles
\def\mcirc{\mathbin{\scalerel*{\circ}{j}}}
\def\msquare{\mathord{\scalerel*{\Box}{gX}}}

\begin{document}

\begin{footnotesize}

\begin{center}
\textbf{MATH-338 Final Cheat Sheet [Linear Regression]}
\end{center}

\begin{flushleft}
\textbf{TERMS}
\end{flushleft}

\par\noindent\rule{\textwidth}{0.4pt}

\begin{flushleft}
\textbf{Direction:} Positive($+\beta_{1}$), Negative($-\beta_{1}$), no association, complex (parabolic).
\textbf{Correlation:} unitless, invariant to linear transformation. Highly susceptible to outliers. Can have strong non linear association but correlation close to 0 (complex).
\textbf{$\mathbf{-1 \le r \le 1}$:} when $r=1$, all points on line with positive slope. When $r=-1$, all points on line with negative slope. Only interpretable for linear.
\textbf{Coefficient of determination:} represents the proportion of variation $y$ that is explained by/accounted for by the model. ANOVA tests whether this proportion is "significant".
\textbf{Assumptions for Linear Regression Inference:} \textbf{1:} linear model appropriate. \textbf{2:} residuals are normally distributed. \textbf{3:} residuals will have $\mu = 0$ and $\sigma = ?$.
\end{flushleft}

\HL

\begin{tabular}{l | l}

\parbox{0.5\textwidth}{

\begin{flushleft}
\textbf{FORMULAS}
\end{flushleft}

\begin{itemize}

\item $y= \beta_{o} + \beta_{1}x + \epsilon$ [standard formula for linear regression]
\item $\epsilon \sim N(0, \sigma)$ [random variable]
\item $r = \frac{1}{n-1}\times \Sigma(\frac{x - \bar{x}}{S_{x}})(\frac{y - \bar{y}}{S_{y}})$ [correlation]
\item $x_{i} > \bar{x} \rightarrow y_{i} > \bar{y}$ [contribution to r is t]\
\item $\hat{y} = b_{o} + b_{1}x$ [least squares regression line]
\item $t = \frac{\text{Statistic - parameter}}{standard error} = \frac{b_{1} - \beta_{1}}{SE_{b_{1}}}$ [$\beta_{1} = 0$]
\item $F_{observed} \sim F(P, n-P-1)$
\item $t_{observed} \sim t(n-p-1)$
\end{itemize}

}

&

\parbox{0.5\textwidth}{
\begin{flushright}
\begin{itemize}

\item $b_{1} = r \times \frac{s_{y}}{s_{x}}$
\item $b_{0} = \bar{y} - b_{1}\bar{x}$
\item $e_{i} = y_{i} - \hat{y}_{i}$ [prediction error (residual)]
\item $r^{2} = \frac{SSM}{SST}$ [coefficient of determination]


\end{itemize}

\begin{flushleft}
\textbf{t-Test}
\end{flushleft}
\begin{itemize}
\item If P-value $\le$ sig level $\implies$ reject H\textsubscript{o} (x\textsubscript{j} is a sig predictor of y)
\item Else, x\textsubscript{j} is not sig, $\therefore$ x\textsubscript{j} does not have linear relation with y
\end{itemize}
\end{flushright}
}

\end{tabular}

\HL


\begin{tabular}{l | l}

\parbox{0.625\textwidth}{

\begin{flushleft}
\textbf{ANOVA}
\end{flushleft}
\begin{flushleft}
 \begin{tabular}{||c | c | c | c | c | c ||} 
 \hline
 Source & DF & $\Sigma \text{ (squares) }$ & $\mu \text{ (squares) }$ & F & $Pr > F$ \\
 \hline
 Model & $p$ & $\sum_{i=1}^{n}(\hat{y}_{i} - \bar{y})^{2}$ & $\frac{SSM}{DFM} = MSM$ & $F_{obs} = \frac{MSM}{MSE}$ & p-value \\ [0.5em]
 \hline
 Error ($e_{i}$) & $n-p-1$ & $\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}$ & $\frac{SSE}{DFE} = MSE$ &  & \\
 \hline
 Total & $n-1$ & $\sum_{i=1}^{n}(y_{i} - \bar{y}_{i})^{2}$ & & & \\
 \hline
\end{tabular}
\begin{itemize}
\item H\textsubscript{0}: $\mu_{1} = \mu_{2} = ... = \mu_{i}$
\item Population mean does not depend on group
\item Population mean of y does not depend on x (H\textsubscript{0}: $\mu_{y|x} = \mu_{y} \implies \beta_{1} = 0$)
\item Reject H\textsubscript{0} : Our model is "significantly better" than null model at explaining changes in y $\implies$ we should use linear model
\item Fail to reject H\textsubscript{0} : Our model is not significantly better than null model $\implies$ should use smaller model (null model)
\end{itemize}
\end{flushleft}
}

&

\parbox{0.5\textwidth}{
\begin{flushleft}
\textbf{SYMBOL CHART}
\end{flushleft}
\begin{flushleft}
 \begin{tabular}{||c | c ||} 
 \hline
 $\beta_{o}$ & y-intercept (b portion in $y=mx+b$) \\
 \hline
 $\beta_{1}$ & slope (m portion in $y=mx+b$) \\ [0.5ex]
 \hline
 $\hat{y}$ & predicted value of y \\
 \hline
 $b_{o}$ & y-intercept for predicted \\
 \hline
 $b_{1}$ & slope for predicted \\
 \hline
 $r$ & coefficient of correlation \\
 \hline
 $x^{\ast}$ & predictor for $\hat{\mu} \text{ and } \hat{y}$ \\
 \hline
\end{tabular}
\end{flushleft}
}
\end{tabular}

\HL

\begin{tabular}{l | l}

\parbox{0.4\textwidth}{

\begin{flushleft}
\textbf{t-Test for Slope}
\end{flushleft}

\begin{itemize}

\item H\textsubscript{0}: $\beta_{1} = 0$ {$t \sim t(n-2)$}
\item H\textsubscript{a}: $\beta \ne = 0$
\item We reject H\textsubscript{0} if slope is not 0, so a linear relationship exists between x and y.
\item Fail to reject H\textsubscript{0} and it is reasonable to believe that slope is 0 and there is no linear relationship between x and y.

\end{itemize}

\begin{flushleft}
\textbf{Mean Response}
\end{flushleft}

\begin{itemize}
\item Model: $\mu_{y|x} = \beta_{0} + \beta_{1}x$
\item $\uparrow$ n $\rightarrow$ $\uparrow$ CI $\downarrow$ relationship
\item CI at $x^{\ast}$ close to $\bar{x} \rightarrow$ narrower
\item CI at $x^{\ast}$ far from $\bar{x} \rightarrow$ wider
\item $PI = \hat{y} \pm t^{\ast\ast} \times SE_{\hat{y}}$
\item We are 95\% confident in our estimate that when x-variable is value of $x^{\ast}$, the population mean of y-variable for a new observation is between lower and upper bound.
\end{itemize}

}

&

\parbox{0.5\textwidth}{

\begin{flushleft}

\textbf{Confidence Interval for Slope}
\begin{itemize}

\item Want to find values of $\beta_{1}$ for which t-statistic is \underline{NOT} in the critical region
\item We are 95\% confident in our estimate that when x-variable increases by 1 unit, the population mean of y-variable increases (1 decreases) by between lower bound and upper bound.

\end{itemize}

\begin{flushleft}
\textbf{MULTIPLE LINEAR REGRESSION}
\end{flushleft}

\begin{flushleft}
\textbf{basic model}
\end{flushleft}
\begin{itemize}
\item $\mu_{y|x} = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{p}x_{p}$ 
\end{itemize}
\begin{flushleft}
\textbf{least squares line}
\end{flushleft}

\begin{itemize}
\item $\hat{y} = b_{0} + b_{1}x_{1} + ... + b_{p}x_{p} \implies y_{i} = \hat{y}_{i} + e_{i}$
\end{itemize}

\begin{flushleft}
\textbf{ANOVA}
\end{flushleft}

\begin{itemize}
\item If p-value $\le$ sig level $\implies$ reject H\textsubscript{o} (our model \underline{is} significantly better than null for prediction)
\item Else, our model is \underline{not significantly} better than the null for predicting
\item Some x-variables may still be important predictors, however "more important" predictors are left out of the model (backward selection)
\end{itemize}

\end{flushleft}

}
\end{tabular}

\end{footnotesize}

\end{document}
